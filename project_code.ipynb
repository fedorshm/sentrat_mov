{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9630664,"sourceType":"datasetVersion","datasetId":5879330},{"sourceId":7494749,"sourceType":"datasetVersion","datasetId":4364005},{"sourceId":9641840,"sourceType":"datasetVersion","datasetId":5887898}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Код разделен на четыре основных этапа:\n    \n    \n1) Работа с данными\n\n2) Обучение классификатора сентимента (настроения)\n\n3) Обучение классификатора рейтинга\n\n4) Оценка работы","metadata":{}},{"cell_type":"markdown","source":"Работа с данными","metadata":{}},{"cell_type":"markdown","source":"Сначала извлечем все данные из текстовых файлов и оформим датасеты","metadata":{}},{"cell_type":"markdown","source":"Сделаем датасет из всех позитивных комментариев для обучения:","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\ndirectory_path = '/kaggle/input/avaliacoes-de-filmes-dataset-imdb/aclImdb/train/pos'\n\ndata = []\n\nfor filename in tqdm(os.listdir(directory_path), desc=\"Обработка\", unit=\"file\"):\n    if filename.endswith('.txt'):\n        # достаем ID и рейтинг из названия файла\n        id_str, rating_str = filename[:-4].split('_')\n        review_id = int(id_str)\n        rating = int(rating_str)\n\n        file_path = os.path.join(directory_path, filename)\n\n        # достанем текст\n        with open(file_path, 'r', encoding='utf-8') as file:\n            review_text = file.read().strip()\n\n        data.append({\n            'ID': review_id,\n            'rating': rating,\n            'text': review_text,\n            'label': 'pos'\n        })\n\n# создадим датафрейм со всеми позитивными тренировочными данными\nreviews_df = pd.DataFrame(data)\nprint(reviews_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:39:04.370244Z","iopub.execute_input":"2024-10-16T13:39:04.370630Z","iopub.status.idle":"2024-10-16T13:40:57.031648Z","shell.execute_reply.started":"2024-10-16T13:39:04.370588Z","shell.execute_reply":"2024-10-16T13:40:57.030713Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Обработка: 100%|██████████| 12500/12500 [01:51<00:00, 112.37file/s]","output_type":"stream"},{"name":"stdout","text":"     ID  rating                                               text label\n0  2714      10  This was one of those wonderful rare moments i...   pos\n1   589      10  Have you seen The Graduate? It was hailed as t...   pos\n2  2211       8  I don't watch a lot of TV, except for The Offi...   pos\n3  2658      10  Kubrick again puts on display his stunning abi...   pos\n4  8929       8  First of all, I liked very much the central id...   pos\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"То же самое для негатива:","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\ndirectory_path = '/kaggle/input/avaliacoes-de-filmes-dataset-imdb/aclImdb/train/neg'\n\ndata = []\n\nfor filename in tqdm(os.listdir(directory_path), desc=\"Обработка\", unit=\"file\"):\n    if filename.endswith('.txt'):\n        id_str, rating_str = filename[:-4].split('_')\n        review_id = int(id_str)\n        rating = int(rating_str)\n\n        file_path = os.path.join(directory_path, filename)\n\n        with open(file_path, 'r', encoding='utf-8') as file:\n            review_text = file.read().strip()\n\n        data.append({\n            'ID': review_id,\n            'rating': rating,\n            'text': review_text,\n            'label': 'neg' \n        })\n\nneg_reviews_df = pd.DataFrame(data)\nprint(neg_reviews_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:40:57.033808Z","iopub.execute_input":"2024-10-16T13:40:57.034480Z","iopub.status.idle":"2024-10-16T13:42:40.768833Z","shell.execute_reply.started":"2024-10-16T13:40:57.034432Z","shell.execute_reply":"2024-10-16T13:42:40.767860Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Обработка: 100%|██████████| 12500/12500 [01:42<00:00, 121.55file/s]","output_type":"stream"},{"name":"stdout","text":"     ID  rating                                               text label\n0  3606       2  This film is the worst film, but it ranks very...   neg\n1  1074       4  I should never have started this film, and sto...   neg\n2  4743       1  I'm here again in your local shopping mall (of...   neg\n3  7628       1  Black and White film. Good photography. Believ...   neg\n4  6812       1  from the start of this movie you soon become a...   neg\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Теперь объединим датафреймы в один тренировочный датафрейм:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ncombined_df = pd.concat([reviews_df, neg_reviews_df], axis=0, ignore_index=True)\n\nprint(combined_df.head())\nprint(f\"Шейп объединенного тренировочного датасета: {combined_df.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:42:40.770045Z","iopub.execute_input":"2024-10-16T13:42:40.770370Z","iopub.status.idle":"2024-10-16T13:42:40.779672Z","shell.execute_reply.started":"2024-10-16T13:42:40.770334Z","shell.execute_reply":"2024-10-16T13:42:40.778733Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"     ID  rating                                               text label\n0  2714      10  This was one of those wonderful rare moments i...   pos\n1   589      10  Have you seen The Graduate? It was hailed as t...   pos\n2  2211       8  I don't watch a lot of TV, except for The Offi...   pos\n3  2658      10  Kubrick again puts on display his stunning abi...   pos\n4  8929       8  First of all, I liked very much the central id...   pos\nШейп объединенного тренировочного датасета: (25000, 4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Теперь проверим датасет на дубликаты","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ncombined_df['id'] = combined_df.index\nduplicates = combined_df[combined_df.duplicated(subset='text', keep=False)]\nsorted_duplicates = duplicates.sort_values(by='text')\n\nprint(sorted_duplicates)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:42:40.781643Z","iopub.execute_input":"2024-10-16T13:42:40.781949Z","iopub.status.idle":"2024-10-16T13:42:40.826388Z","shell.execute_reply.started":"2024-10-16T13:42:40.781909Z","shell.execute_reply":"2024-10-16T13:42:40.825561Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"         ID  rating                                               text label  \\\n23479  4102       4  'Dead Letter Office' is a low-budget film abou...   neg   \n14197   985       4  'Dead Letter Office' is a low-budget film abou...   neg   \n6663   9319       8  .......Playing Kaddiddlehopper, Col San Fernan...   pos   \n2441   6069       8  .......Playing Kaddiddlehopper, Col San Fernan...   pos   \n19911  7287       2  <br /><br />Back in his youth, the old man had...   neg   \n...     ...     ...                                                ...   ...   \n24501  6818       3  in this movie, joe pesci slams dunks a basketb...   neg   \n8660   8657       9  it's amazing that so many people that i know h...   pos   \n254    8654       9  it's amazing that so many people that i know h...   pos   \n18898  5085       1  this movie begins with an ordinary funeral... ...   neg   \n23148  6642       1  this movie begins with an ordinary funeral... ...   neg   \n\n          id  \n23479  23479  \n14197  14197  \n6663    6663  \n2441    2441  \n19911  19911  \n...      ...  \n24501  24501  \n8660    8660  \n254      254  \n18898  18898  \n23148  23148  \n\n[188 rows x 5 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Дропнем дубликаты","metadata":{}},{"cell_type":"code","source":"df_unique = combined_df.drop_duplicates(subset='text')\nprint(df_unique)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:42:40.827468Z","iopub.execute_input":"2024-10-16T13:42:40.827775Z","iopub.status.idle":"2024-10-16T13:42:40.840679Z","shell.execute_reply.started":"2024-10-16T13:42:40.827744Z","shell.execute_reply":"2024-10-16T13:42:40.839769Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"          ID  rating                                               text label  \\\n0       2714      10  This was one of those wonderful rare moments i...   pos   \n1        589      10  Have you seen The Graduate? It was hailed as t...   pos   \n2       2211       8  I don't watch a lot of TV, except for The Offi...   pos   \n3       2658      10  Kubrick again puts on display his stunning abi...   pos   \n4       8929       8  First of all, I liked very much the central id...   pos   \n...      ...     ...                                                ...   ...   \n24995   1072       4  The first hour of the movie was boring as hell...   neg   \n24996  11693       4  A fun concept, but poorly executed. Except for...   neg   \n24997   1550       1  I honestly don't understand how tripe like thi...   neg   \n24998  12186       1  This remake of the 1962 orginal film'o the boo...   neg   \n24999    501       2  La Sanguisuga Conduce la Danza, or The Bloodsu...   neg   \n\n          id  \n0          0  \n1          1  \n2          2  \n3          3  \n4          4  \n...      ...  \n24995  24995  \n24996  24996  \n24997  24997  \n24998  24998  \n24999  24999  \n\n[24904 rows x 5 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Сохраним, чтобы не потерять","metadata":{}},{"cell_type":"code","source":"df_unique.to_csv('/kaggle/working/combined_train.csv', encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:42:40.841813Z","iopub.execute_input":"2024-10-16T13:42:40.842114Z","iopub.status.idle":"2024-10-16T13:42:42.222362Z","shell.execute_reply.started":"2024-10-16T13:42:40.842084Z","shell.execute_reply":"2024-10-16T13:42:42.221351Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Теперь проделаем все то же саме для тестового датасета","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\ndirectory_path = '/kaggle/input/avaliacoes-de-filmes-dataset-imdb/aclImdb/test/pos'\n\ndata = []\n\nfor filename in tqdm(os.listdir(directory_path), desc=\"Обработка\", unit=\"file\"):\n    if filename.endswith('.txt'):\n        # достаем ID и рейтинг из названия файла\n        id_str, rating_str = filename[:-4].split('_')\n        review_id = int(id_str)\n        rating = int(rating_str)\n\n        file_path = os.path.join(directory_path, filename)\n\n        # достанем текст\n        with open(file_path, 'r', encoding='utf-8') as file:\n            review_text = file.read().strip()\n\n        data.append({\n            'ID': review_id,\n            'rating': rating,\n            'text': review_text,\n            'label': 'pos'\n        })\n\n# создадим датафрейм со всеми позитивными тренировочными данными\nreviews_df = pd.DataFrame(data)\nprint(reviews_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:42:46.144052Z","iopub.execute_input":"2024-10-16T13:42:46.144433Z","iopub.status.idle":"2024-10-16T13:44:32.803640Z","shell.execute_reply.started":"2024-10-16T13:42:46.144398Z","shell.execute_reply":"2024-10-16T13:44:32.802771Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Обработка: 100%|██████████| 12500/12500 [01:45<00:00, 118.29file/s]","output_type":"stream"},{"name":"stdout","text":"     ID  rating                                               text label\n0   589      10  I've Seen The Beginning Of The Muppet Movie, B...   pos\n1  6451       9  If it had been made 2 years later it would hav...   pos\n2  2750       8  Very good \"Precoder\" starring Dick Barthelmess...   pos\n3  5746      10  A young man discovers that life is precious af...   pos\n4  2658      10  I'm always surprised, given that the famous ti...   pos\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\ndirectory_path = '/kaggle/input/avaliacoes-de-filmes-dataset-imdb/aclImdb/test/neg'\n\ndata = []\n\nfor filename in tqdm(os.listdir(directory_path), desc=\"Обработка\", unit=\"file\"):\n    if filename.endswith('.txt'):\n        id_str, rating_str = filename[:-4].split('_')\n        review_id = int(id_str)\n        rating = int(rating_str)\n\n        file_path = os.path.join(directory_path, filename)\n\n        with open(file_path, 'r', encoding='utf-8') as file:\n            review_text = file.read().strip()\n\n        data.append({\n            'ID': review_id,\n            'rating': rating,\n            'text': review_text,\n            'label': 'neg' \n        })\n\nneg_reviews_df = pd.DataFrame(data)\nprint(neg_reviews_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:44:32.805247Z","iopub.execute_input":"2024-10-16T13:44:32.805559Z","iopub.status.idle":"2024-10-16T13:46:20.293677Z","shell.execute_reply.started":"2024-10-16T13:44:32.805527Z","shell.execute_reply":"2024-10-16T13:46:20.292789Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Обработка: 100%|██████████| 12500/12500 [01:46<00:00, 117.30file/s]","output_type":"stream"},{"name":"stdout","text":"     ID  rating                                               text label\n0   565       2  Committed doom and gloomer Peter Watkins goes ...   neg\n1  6496       4  Most critics have written devastating about th...   neg\n2  4408       1  Did I waste my time. This is very pretentious ...   neg\n3  7628       1  What a stinker!!! I swear this movie was writt...   neg\n4  6812       1  Ever had one of those nights when you couldn't...   neg\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ncombined_df = pd.concat([reviews_df, neg_reviews_df], axis=0, ignore_index=True)\n\nprint(combined_df.head())\nprint(f\"Шейп объединенного тренировочного датасета: {combined_df.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:20.294979Z","iopub.execute_input":"2024-10-16T13:46:20.295361Z","iopub.status.idle":"2024-10-16T13:46:20.306793Z","shell.execute_reply.started":"2024-10-16T13:46:20.295317Z","shell.execute_reply":"2024-10-16T13:46:20.305911Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"     ID  rating                                               text label\n0   589      10  I've Seen The Beginning Of The Muppet Movie, B...   pos\n1  6451       9  If it had been made 2 years later it would hav...   pos\n2  2750       8  Very good \"Precoder\" starring Dick Barthelmess...   pos\n3  5746      10  A young man discovers that life is precious af...   pos\n4  2658      10  I'm always surprised, given that the famous ti...   pos\nШейп объединенного тренировочного датасета: (25000, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ncombined_df['id'] = combined_df.index\nduplicates = combined_df[combined_df.duplicated(subset='text', keep=False)]\nsorted_duplicates = duplicates.sort_values(by='text')\n\nprint(sorted_duplicates)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:20.308919Z","iopub.execute_input":"2024-10-16T13:46:20.309205Z","iopub.status.idle":"2024-10-16T13:46:20.344085Z","shell.execute_reply.started":"2024-10-16T13:46:20.309174Z","shell.execute_reply":"2024-10-16T13:46:20.343069Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"          ID  rating                                               text label  \\\n16915   2183       2  \"Go Fish\" garnered Rose Troche rightly or wron...   neg   \n12871  10977       2  \"Go Fish\" garnered Rose Troche rightly or wron...   neg   \n18640   6089       3  \"Three\" is a seriously dumb shipwreck movie. M...   neg   \n23145   4129       3  \"Three\" is a seriously dumb shipwreck movie. M...   neg   \n20446   3124       4  (Spoilers)<br /><br />Oh sure it's based on Mo...   neg   \n...      ...     ...                                                ...   ...   \n17077   4125       1  this is the worst film I've seen in a long lon...   neg   \n16367   1966       1  this movie sucks. did anyone notice that the e...   neg   \n13630   3374       1  this movie sucks. did anyone notice that the e...   neg   \n8715    2975      10  when I first heard about this movie, I noticed...   pos   \n2090    2971      10  when I first heard about this movie, I noticed...   pos   \n\n          id  \n16915  16915  \n12871  12871  \n18640  18640  \n23145  23145  \n20446  20446  \n...      ...  \n17077  17077  \n16367  16367  \n13630  13630  \n8715    8715  \n2090    2090  \n\n[390 rows x 5 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_unique = combined_df.drop_duplicates(subset='text')\nprint(df_unique)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:20.345906Z","iopub.execute_input":"2024-10-16T13:46:20.346812Z","iopub.status.idle":"2024-10-16T13:46:20.369076Z","shell.execute_reply.started":"2024-10-16T13:46:20.346766Z","shell.execute_reply":"2024-10-16T13:46:20.367551Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"          ID  rating                                               text label  \\\n0        589      10  I've Seen The Beginning Of The Muppet Movie, B...   pos   \n1       6451       9  If it had been made 2 years later it would hav...   pos   \n2       2750       8  Very good \"Precoder\" starring Dick Barthelmess...   pos   \n3       5746      10  A young man discovers that life is precious af...   pos   \n4       2658      10  I'm always surprised, given that the famous ti...   pos   \n...      ...     ...                                                ...   ...   \n24995   6301       4  This is one of those inoffensive and mildly en...   neg   \n24996     23       4  When people say children are annoying u think ...   neg   \n24997   5134       3  OK, I don't want to upset anyone who enjoyed t...   neg   \n24998   6660       2  Words can scarcely describe this movie. Loaded...   neg   \n24999  12186       1  I watched this movie last night, i'm a huge fa...   neg   \n\n          id  \n0          0  \n1          1  \n2          2  \n3          3  \n4          4  \n...      ...  \n24995  24995  \n24996  24996  \n24997  24997  \n24998  24998  \n24999  24999  \n\n[24801 rows x 5 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_unique.to_csv('/kaggle/working/combined_test.csv', encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:20.370190Z","iopub.execute_input":"2024-10-16T13:46:20.370562Z","iopub.status.idle":"2024-10-16T13:46:21.710840Z","shell.execute_reply.started":"2024-10-16T13:46:20.370529Z","shell.execute_reply":"2024-10-16T13:46:21.710011Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Сделаем один общий датасет","metadata":{}},{"cell_type":"code","source":"df_1 = pd.read_csv('/kaggle/working/combined_train.csv')\ndf_2 = pd.read_csv('/kaggle/working/combined_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:47.449241Z","iopub.execute_input":"2024-10-16T13:46:47.450015Z","iopub.status.idle":"2024-10-16T13:46:48.077783Z","shell.execute_reply.started":"2024-10-16T13:46:47.449975Z","shell.execute_reply":"2024-10-16T13:46:48.076998Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"combined_df_full = pd.concat([df_1, df_2], axis=0, ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:49.608806Z","iopub.execute_input":"2024-10-16T13:46:49.609613Z","iopub.status.idle":"2024-10-16T13:46:49.617017Z","shell.execute_reply.started":"2024-10-16T13:46:49.609575Z","shell.execute_reply":"2024-10-16T13:46:49.616212Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"combined_df_full","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:51.085646Z","iopub.execute_input":"2024-10-16T13:46:51.086021Z","iopub.status.idle":"2024-10-16T13:46:51.102562Z","shell.execute_reply.started":"2024-10-16T13:46:51.085986Z","shell.execute_reply":"2024-10-16T13:46:51.101639Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0     ID  rating  \\\n0               0   2714      10   \n1               1    589      10   \n2               2   2211       8   \n3               3   2658      10   \n4               4   8929       8   \n...           ...    ...     ...   \n49700       24995   6301       4   \n49701       24996     23       4   \n49702       24997   5134       3   \n49703       24998   6660       2   \n49704       24999  12186       1   \n\n                                                    text label     id  \n0      This was one of those wonderful rare moments i...   pos      0  \n1      Have you seen The Graduate? It was hailed as t...   pos      1  \n2      I don't watch a lot of TV, except for The Offi...   pos      2  \n3      Kubrick again puts on display his stunning abi...   pos      3  \n4      First of all, I liked very much the central id...   pos      4  \n...                                                  ...   ...    ...  \n49700  This is one of those inoffensive and mildly en...   neg  24995  \n49701  When people say children are annoying u think ...   neg  24996  \n49702  OK, I don't want to upset anyone who enjoyed t...   neg  24997  \n49703  Words can scarcely describe this movie. Loaded...   neg  24998  \n49704  I watched this movie last night, i'm a huge fa...   neg  24999  \n\n[49705 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>ID</th>\n      <th>rating</th>\n      <th>text</th>\n      <th>label</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2714</td>\n      <td>10</td>\n      <td>This was one of those wonderful rare moments i...</td>\n      <td>pos</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>589</td>\n      <td>10</td>\n      <td>Have you seen The Graduate? It was hailed as t...</td>\n      <td>pos</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2211</td>\n      <td>8</td>\n      <td>I don't watch a lot of TV, except for The Offi...</td>\n      <td>pos</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2658</td>\n      <td>10</td>\n      <td>Kubrick again puts on display his stunning abi...</td>\n      <td>pos</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>8929</td>\n      <td>8</td>\n      <td>First of all, I liked very much the central id...</td>\n      <td>pos</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49700</th>\n      <td>24995</td>\n      <td>6301</td>\n      <td>4</td>\n      <td>This is one of those inoffensive and mildly en...</td>\n      <td>neg</td>\n      <td>24995</td>\n    </tr>\n    <tr>\n      <th>49701</th>\n      <td>24996</td>\n      <td>23</td>\n      <td>4</td>\n      <td>When people say children are annoying u think ...</td>\n      <td>neg</td>\n      <td>24996</td>\n    </tr>\n    <tr>\n      <th>49702</th>\n      <td>24997</td>\n      <td>5134</td>\n      <td>3</td>\n      <td>OK, I don't want to upset anyone who enjoyed t...</td>\n      <td>neg</td>\n      <td>24997</td>\n    </tr>\n    <tr>\n      <th>49703</th>\n      <td>24998</td>\n      <td>6660</td>\n      <td>2</td>\n      <td>Words can scarcely describe this movie. Loaded...</td>\n      <td>neg</td>\n      <td>24998</td>\n    </tr>\n    <tr>\n      <th>49704</th>\n      <td>24999</td>\n      <td>12186</td>\n      <td>1</td>\n      <td>I watched this movie last night, i'm a huge fa...</td>\n      <td>neg</td>\n      <td>24999</td>\n    </tr>\n  </tbody>\n</table>\n<p>49705 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Проверим дубликаты","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ncombined_df_full['id'] = combined_df_full.index\nduplicates = combined_df_full[combined_df_full.duplicated(subset='text', keep=False)]\nsorted_duplicates = duplicates.sort_values(by='text')\n\nprint(sorted_duplicates)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:54.816929Z","iopub.execute_input":"2024-10-16T13:46:54.817744Z","iopub.status.idle":"2024-10-16T13:46:54.877227Z","shell.execute_reply.started":"2024-10-16T13:46:54.817682Z","shell.execute_reply":"2024-10-16T13:46:54.876198Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"       Unnamed: 0     ID  rating  \\\n48067       23329     69       4   \n23105       23182  12298       4   \n20777       20833   8091       4   \n45265       20482   9507       4   \n17948       17987   2534       3   \n...           ...    ...     ...   \n47781       23039   3886       3   \n7726         7737   2405      10   \n31715        6826   9865      10   \n40697       15867   1537       2   \n22945       23019  10203       2   \n\n                                                    text label     id  \n48067  \"Witchery\" might just be the most incoherent a...   neg  48067  \n23105  \"Witchery\" might just be the most incoherent a...   neg  23105  \n20777  (This is a review of the later English release...   neg  20777  \n45265  (This is a review of the later English release...   neg  45265  \n17948  * Some spoilers *<br /><br />This movie is som...   neg  17948  \n...                                                  ...   ...    ...  \n47781  With Knightly and O'Tool as the leads, this fi...   neg  47781  \n7726   Wow, I can't believe i'm the first and only on...   pos   7726  \n31715  Wow, I can't believe i'm the first and only on...   pos  31715  \n40697  well, the writing was very sloppy, the directi...   neg  40697  \n22945  well, the writing was very sloppy, the directi...   neg  22945  \n\n[246 rows x 6 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Дропнем дубликаты","metadata":{}},{"cell_type":"code","source":"df_unique = combined_df_full.drop_duplicates(subset='text')\nprint(df_unique)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:46:58.519824Z","iopub.execute_input":"2024-10-16T13:46:58.520196Z","iopub.status.idle":"2024-10-16T13:46:58.544478Z","shell.execute_reply.started":"2024-10-16T13:46:58.520162Z","shell.execute_reply":"2024-10-16T13:46:58.543636Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"       Unnamed: 0     ID  rating  \\\n0               0   2714      10   \n1               1    589      10   \n2               2   2211       8   \n3               3   2658      10   \n4               4   8929       8   \n...           ...    ...     ...   \n49700       24995   6301       4   \n49701       24996     23       4   \n49702       24997   5134       3   \n49703       24998   6660       2   \n49704       24999  12186       1   \n\n                                                    text label     id  \n0      This was one of those wonderful rare moments i...   pos      0  \n1      Have you seen The Graduate? It was hailed as t...   pos      1  \n2      I don't watch a lot of TV, except for The Offi...   pos      2  \n3      Kubrick again puts on display his stunning abi...   pos      3  \n4      First of all, I liked very much the central id...   pos      4  \n...                                                  ...   ...    ...  \n49700  This is one of those inoffensive and mildly en...   neg  49700  \n49701  When people say children are annoying u think ...   neg  49701  \n49702  OK, I don't want to upset anyone who enjoyed t...   neg  49702  \n49703  Words can scarcely describe this movie. Loaded...   neg  49703  \n49704  I watched this movie last night, i'm a huge fa...   neg  49704  \n\n[49582 rows x 6 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Итого у нас 49582 строки","metadata":{}},{"cell_type":"markdown","source":"Посмотрим, как там с распределением классов","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nlabel_counts = df_unique['label'].value_counts()\n\nprint(label_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:47:02.905789Z","iopub.execute_input":"2024-10-16T13:47:02.906158Z","iopub.status.idle":"2024-10-16T13:47:02.919472Z","shell.execute_reply.started":"2024-10-16T13:47:02.906122Z","shell.execute_reply":"2024-10-16T13:47:02.918690Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"label\npos    24884\nneg    24698\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Как было поровну, так и осталось (в пределах погрешности)","metadata":{}},{"cell_type":"markdown","source":"Теперь посмотрим, сколько токенов у нас","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BertTokenizer\nfrom tqdm import tqdm\n\ndf = df_unique\n\n#возьмем токенизатор от берта\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntqdm.pandas(desc=\"Токенизация\")\n\ndef count_tokens(text):\n    tokens = tokenizer.tokenize(text)\n    return len(tokens)\n\ndf['token_count'] = df['text'].progress_apply(count_tokens)\n\n# Calculate stats\nmax_tokens = df['token_count'].max()\nmin_tokens = df['token_count'].min()\naverage_tokens = df['token_count'].mean()\ntotal_tokens = df['token_count'].sum()\n\nprint(f\"Максимальное кол-во токенов в отзыве: {max_tokens}\")\nprint(f\"Минимальное кол-во токенов в отзыве: {min_tokens}\")\nprint(f\"Среднее кол-во токенов в отзыве: {average_tokens:.2f}\")\nprint(f\"Всего токенов в датасете: {total_tokens}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:47:09.246284Z","iopub.execute_input":"2024-10-16T13:47:09.246962Z","iopub.status.idle":"2024-10-16T13:54:07.625097Z","shell.execute_reply.started":"2024-10-16T13:47:09.246921Z","shell.execute_reply":"2024-10-16T13:54:07.624161Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf334e22be71425e9f0840535abf0b62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc1fcb5fba745ee811645d0c45cf92d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4977496e4490499d9f570dec764cef64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc60a7f01874d9599a62db44c162bbd"}},"metadata":{}},{"name":"stderr","text":"Токенизация: 100%|██████████| 49582/49582 [06:52<00:00, 120.27it/s]","output_type":"stream"},{"name":"stdout","text":"Максимальное кол-во токенов в отзыве: 3155\nМинимальное кол-во токенов в отзыве: 8\nСреднее кол-во токенов в отзыве: 308.57\nВсего токенов в датасете: 15299488\n","output_type":"stream"},{"name":"stderr","text":"\n/tmp/ipykernel_30/3633013716.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['token_count'] = df['text'].progress_apply(count_tokens)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Посмотрим, сколько уникальных значений рейтингов (согласно описанию от создателей датасета, должны быть значения 1, 2, 3, 4, 7, 8, 9, 10 (без 5 и 6).","metadata":{}},{"cell_type":"code","source":"unique_ratings = df_unique['rating'].unique()\nsorted_unique_ratings = sorted(unique_ratings)\n\nprint(sorted_unique_ratings)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:56:15.825051Z","iopub.execute_input":"2024-10-16T13:56:15.826065Z","iopub.status.idle":"2024-10-16T13:56:15.833851Z","shell.execute_reply.started":"2024-10-16T13:56:15.826014Z","shell.execute_reply":"2024-10-16T13:56:15.832941Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[1, 2, 3, 4, 7, 8, 9, 10]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Все верно.","metadata":{}},{"cell_type":"markdown","source":"Теперь разделим нас огромный датасет на обучающую и тестовую выборки","metadata":{}},{"cell_type":"markdown","source":"Примечание: авторы датасета предлагают 25000 текстов на трейн и 25000 текстов на тест. Кажется, что лучше будет пойти по классическому пути разделения выборки на кусочки 0.8 к 0.2, чтобы было больше данных для обучения.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = df_unique\nprint(df.head())\n\n# Разделим в соотношении 0.8 на трейн к 0.2 на тест\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=0.2, \n    stratify=df['label'], \n    random_state=42\n)\n\nprint('Распределение классов для трейна:')\nprint(train_df['label'].value_counts(normalize=True))\n\nprint('Распределение классов для теста:')\nprint(test_df['label'].value_counts(normalize=True))\n\n# Сохраним\ntrain_df.to_csv('/kaggle/working/train_reviews.csv', index=False)\ntest_df.to_csv('/kaggle/working/test_reviews.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T13:56:23.444203Z","iopub.execute_input":"2024-10-16T13:56:23.444628Z","iopub.status.idle":"2024-10-16T13:56:26.843169Z","shell.execute_reply.started":"2024-10-16T13:56:23.444587Z","shell.execute_reply":"2024-10-16T13:56:26.842385Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"   Unnamed: 0    ID  rating  \\\n0           0  2714      10   \n1           1   589      10   \n2           2  2211       8   \n3           3  2658      10   \n4           4  8929       8   \n\n                                                text label  id  token_count  \n0  This was one of those wonderful rare moments i...   pos   0          208  \n1  Have you seen The Graduate? It was hailed as t...   pos   1          933  \n2  I don't watch a lot of TV, except for The Offi...   pos   2          354  \n3  Kubrick again puts on display his stunning abi...   pos   3          163  \n4  First of all, I liked very much the central id...   pos   4          227  \nРаспределение классов для трейна:\nlabel\npos    0.501878\nneg    0.498122\nName: proportion, dtype: float64\nРаспределение классов для теста:\nlabel\npos    0.501865\nneg    0.498135\nName: proportion, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Распределение классов - примерно по 50 процентов для каждой выборки, как и должно быть.","metadata":{}},{"cell_type":"markdown","source":"Обучение классификаторов","metadata":{}},{"cell_type":"markdown","source":"Было решено обучить две модели: одна для предсказания сентимента отзыва, вторая - для предсказания класса.\n\nДля обоих классификаторов было решено выбрать модель distilroberta-base. Модели roberta отлично улавливают контекст и информацию в текстовых данных. У нас отзывы к фильмам, где может быть очень важно улавливать контекстную информацию, потому что отзывы могут быть очень разнообразными в плане формального представления, что не всегда сможет уловить чисто формальная модель. Как пример, два отзыва на английском:\n\n1) This movie is shit. (= фильм очень плохой, отзыв негативный)\n\n2) This movie is the real shit. (= фильм очень хороший, отзыв позитивный)\n\nТрансформенные модели, вероятно, смогли бы уловить разницу.\n\nТакже, была выбрана именно дистиллированная роберта, так как она меньше по размеру, что позволит ее развернуть на нашем сервисе с ограничением ресурсов для бесплатного использования.","metadata":{}},{"cell_type":"markdown","source":"В процессе обучения также применялся чанкинг, так как некоторые тексты значительно длиннее 512 токенов, которые может принять роберта на обучение в одном примере. ","metadata":{}},{"cell_type":"markdown","source":"Код для обучения модели для предсказания рейтингов отзывов по тексту:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# подгружаем наши уже разделенные на трейн и тест датасеты\ntrain_df = pd.read_csv('/kaggle/working/train_reviews.csv')\ntest_df = pd.read_csv('/kaggle/working/test_reviews.csv')\n\n# подгрузим токенизатор дистиллированной роберты\ntokenizer = RobertaTokenizer.from_pretrained('distilbert/distilroberta-base')\n\n#функция для чанкинга с максимальной длиной чанка 512 и страйдом 256\nclass SentimentChunkedDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, stride):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.stride = stride\n        self.samples = self.create_samples(dataframe)\n\n    def create_samples(self, dataframe):\n        samples = []\n        for idx in range(len(dataframe)):\n            text = dataframe.iloc[idx, dataframe.columns.get_loc('text')]\n            rating = dataframe.iloc[idx, dataframe.columns.get_loc('rating')]\n            label = {1: 0, 2: 1, 3: 2, 4: 3, 7: 4, 8: 5, 9: 6, 10: 7}[rating]\n            encodings = self.tokenizer(text, truncation=False, return_tensors=\"pt\")\n            token_count = encodings.input_ids.size(1)\n            \n            if token_count > self.max_len:\n                for start in range(0, token_count, self.stride):\n                    end = min(start + self.max_len, token_count)\n                    chunk_encoding = {key: val[:, start:end] for key, val in encodings.items()}\n                    chunk_encoding['labels'] = torch.tensor(label)\n                    samples.append(chunk_encoding)\n            else:\n                encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n                encodings['labels'] = torch.tensor(label)\n                samples.append(encodings)\n        return samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return {key: val.squeeze(0) for key, val in self.samples[idx].items()}\n\nmax_len = 512\nstride = max_len // 2\n\n#подготавливаем датасеты - на трейн идет весь трейн, а на валидацию весь тест\ntrain_dataset = SentimentChunkedDataset(train_df, tokenizer, max_len, stride)\nval_dataset = SentimentChunkedDataset(test_df, tokenizer, max_len, stride)\n\n# Будем смотреть на accuracy, precision, recall и f1 во время обучения\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    labels = p.label_ids\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n    acc = accuracy_score(labels, preds)\n    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n\n# подгружаем модель и обозначаем кол-во классов (рейтингов) - 8\nmodel = RobertaForSequenceClassification.from_pretrained('distilbert/distilroberta-base', num_labels=8)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# обозначим гиперпараметры для обучения. предварительно ставим 15 эпох, чтобы смотреть за динамикой модели.\n# сохраняем только одну лучшую по accuracy модель, чтобы экономить место и в любой момент прервать обучение\ntraining_args = TrainingArguments(\n    output_dir='.',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=1,\n    metric_for_best_model='accuracy',\n    load_best_model_at_end=True,\n    num_train_epochs=15,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    learning_rate=5e-5,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='.',\n    logging_steps=10,\n    report_to='none'\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset, \n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n\ntrainer.save_model('.')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T01:56:55.983185Z","iopub.execute_input":"2024-10-16T01:56:55.983768Z","iopub.status.idle":"2024-10-16T03:42:30.244243Z","shell.execute_reply.started":"2024-10-16T01:56:55.983691Z","shell.execute_reply":"2024-10-16T03:42:30.242878Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845e6a2ba3af4c74875ab3051e98e20d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb71d56d4ec4d4e8ad1128403c74a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8829a5689f22480dbcf105b77cabe97d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450c1a87dc2f4e92a3ac21f225a8359d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad6f983f597f4e26977bb811988bc024"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nToken indices sequence length is longer than the specified maximum sequence length for this model (1403 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aec10d582c34f00b414db6a5cfb34d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3335' max='12435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 3335/12435 1:40:56 < 4:35:35, 0.55 it/s, Epoch 4.02/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.270000</td>\n      <td>1.250743</td>\n      <td>0.507428</td>\n      <td>0.501359</td>\n      <td>0.439126</td>\n      <td>0.433963</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.161000</td>\n      <td>1.226578</td>\n      <td>0.508253</td>\n      <td>0.494072</td>\n      <td>0.445218</td>\n      <td>0.440419</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.077200</td>\n      <td>1.285691</td>\n      <td>0.500000</td>\n      <td>0.492454</td>\n      <td>0.456228</td>\n      <td>0.451379</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.852900</td>\n      <td>1.343958</td>\n      <td>0.508779</td>\n      <td>0.470395</td>\n      <td>0.464953</td>\n      <td>0.462921</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 101\u001b[0m\n\u001b[1;32m     90\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     91\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     92\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Evaluate model on validation dataset (here it is the same as the test dataset used for validation)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(val_dataset)  \u001b[38;5;66;03m# This is effectively the same evaluation as during training epochs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Обучение было прервано на 4 эпохе, так как уже на 3 начинается переобучение, и качество не растет значительно. В результате других попыток с разным кол-вом данных был такой же результат.","metadata":{}},{"cell_type":"markdown","source":"Сохраним модель в наш репозиторий на HuggingFace, чтобы пользоваться позже.","metadata":{}},{"cell_type":"code","source":"#входим в хаггинг фейс\nfrom huggingface_hub import login\nfrom huggingface_hub import HfApi, HfFolder, Repository\nimport os\n\nhf_api_token = os.getenv('') \nlogin(token=hf_api_token)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T03:42:51.336508Z","iopub.execute_input":"2024-10-16T03:42:51.337251Z","iopub.status.idle":"2024-10-16T03:42:51.365599Z","shell.execute_reply.started":"2024-10-16T03:42:51.337211Z","shell.execute_reply":"2024-10-16T03:42:51.364782Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb3933219024281ad10e17b19dc7cd3"}},"metadata":{}}]},{"cell_type":"code","source":"# Сохраняем. Модель доступна в указанном репозитории\nrepo_name = \"Gnider/roberta_dist_rat\"\n\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T03:43:47.351464Z","iopub.execute_input":"2024-10-16T03:43:47.351934Z","iopub.status.idle":"2024-10-16T03:44:20.051796Z","shell.execute_reply.started":"2024-10-16T03:43:47.351895Z","shell.execute_reply":"2024-10-16T03:44:20.050601Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a337d678afec4394b733e90a7807b762"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a7c8e6c42e443e8139ff27ac9fab81"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Gnider/roberta_dist_rat/commit/9949d2f585a98696e7b738fe81e4e72b9f717844', commit_message='Upload tokenizer', commit_description='', oid='9949d2f585a98696e7b738fe81e4e72b9f717844', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Gnider/roberta_dist_rat', endpoint='https://huggingface.co', repo_type='model', repo_id='Gnider/roberta_dist_rat'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"Как можно заметить, по метрике accuracy удалось достичь около 50% до того, как началось переобучение. Это немного, и еле затрагивает порог полезности (если вообще затрагивает). Кажется, что есть смысл провести эксперимент на точность в топ-n предсказаниях (например, какова будет точность, если одно из двух предсказаний истинно?), ведь оценка фильма человеком - очень субъективная вещь. Допустим,человек, дающий очень плохой отзыв, мог поставить 1 или 2, и обе эти оценки - показатели очень плохого фильма. Так же и с очень хорошим фильмом. Эксперименты будут ниже после обучения модели для сентимента.","metadata":{}},{"cell_type":"markdown","source":"Обучение модели для классификации сентимента","metadata":{}},{"cell_type":"markdown","source":"В сущности код такой же, как и для рейтингов, за исключением того, что теперь у нас два класса - позитив и негатив.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ntrain_df = pd.read_csv('/kaggle/working/train_reviews.csv')\ntest_df = pd.read_csv('/kaggle/working/test_reviews.csv')\n\ntokenizer = RobertaTokenizer.from_pretrained('distilbert/distilroberta-base')\n\nclass SentimentChunkedDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, stride):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.stride = stride\n        self.samples = self.create_samples(dataframe)\n\n    def create_samples(self, dataframe):\n        samples = []\n        for idx in range(len(dataframe)):\n            text = dataframe.iloc[idx, dataframe.columns.get_loc('text')]\n            label_text = dataframe.iloc[idx, dataframe.columns.get_loc('label')]\n            label = 0 if label_text == 'neg' else 1\n            encodings = self.tokenizer(text, truncation=False, return_tensors=\"pt\")\n            token_count = encodings.input_ids.size(1)\n            \n            if token_count > self.max_len:\n                for start in range(0, token_count, self.stride):\n                    end = min(start + self.max_len, token_count)\n                    chunk_encoding = {key: val[:, start:end] for key, val in encodings.items()}\n                    chunk_encoding['labels'] = torch.tensor(label)\n                    samples.append(chunk_encoding)\n            else:\n                encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n                encodings['labels'] = torch.tensor(label)\n                samples.append(encodings)\n        return samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return {key: val.squeeze(0) for key, val in self.samples[idx].items()}\n\nmax_len = 512\nstride = max_len // 2\n\ntrain_dataset = SentimentChunkedDataset(train_df, tokenizer, max_len, stride)\nval_dataset = SentimentChunkedDataset(test_df, tokenizer, max_len, stride)\n\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    labels = p.label_ids\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n\nmodel = RobertaForSequenceClassification.from_pretrained('distilbert/distilroberta-base', num_labels=2)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir='.',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=1,\n    metric_for_best_model='accuracy',\n    load_best_model_at_end=True,\n    num_train_epochs=4,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    learning_rate=5e-5,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='.',\n    logging_steps=10,\n    report_to='none'\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset, \n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\ntrainer.save_model('.')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Так же сохраним на HuggingFace","metadata":{}},{"cell_type":"code","source":"#входим в хаггинг фейс\nfrom huggingface_hub import login\nfrom huggingface_hub import HfApi, HfFolder, Repository\nimport os\n\nhf_api_token = os.getenv('') \nlogin(token=hf_api_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Сохраняем. Модель доступна в указанном репозитории\nrepo_name = \"Gnider/roberta_dist_sent\"\n\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"К сожалению, лог обучения модели для сентимента не сохранился. Но это не беда, потому что мы можем подгрузить нашу модель с хаггинг фейса, куда мы ее сохранили после обучения, и проверить на тестовом датасете, что мы и сделаем в следующем разделе - оценка моделей.","metadata":{}},{"cell_type":"markdown","source":"Оценка моделей","metadata":{}},{"cell_type":"markdown","source":"Проверим предсказания для модели рейтингов:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\n\n# Подгружаем наш тестовый датасет (который, кстати, и использовался во время валидации в обучении, но здесь мы делаем это отдельно)\ntest_df = pd.read_csv('/kaggle/working/test_reviews.csv')\n\n# подгружаем токенизатор и модель с нашей репы в Хаггинг Фейс\nmodel_name = 'Gnider/roberta_dist_rat'\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\n\n# Check device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Data preparation\nclass TestDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.dataframe = dataframe\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['text']\n        label = self.dataframe.iloc[idx]['rating']\n        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n        encodings['labels'] = torch.tensor(label, dtype=torch.long)\n        return {key: val.squeeze(0) for key, val in encodings.items()}\n\nmax_len = 512\ntest_dataset = TestDataset(test_df, tokenizer, max_len)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\ndef eval_model(test_loader, model, top_k=1):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels']\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            probabilities = torch.softmax(outputs.logits, dim=-1)\n            top_probs, top_preds = probabilities.topk(top_k, dim=-1)\n\n            all_probs.extend(top_probs.cpu().numpy())\n            all_preds.extend(top_preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    correct_count = 0\n    for preds, label in zip(all_preds, all_labels):\n        if label in preds:\n            correct_count += 1\n\n    accuracy = correct_count / len(all_labels)\n    return accuracy, all_preds, all_probs, all_labels\n\n# Проведем 3 эксперимента:\n#точность топ-1 (один предсказанный рейтинг) - по идее должно быть около 0.5, как в обучении\ntop_1_accuracy, top_1_preds, top_1_probs, actual_labels = eval_model(test_loader, model, top_k=1)\n# точность топ-2 - точность по двум наиболее вероятным предсказанным рейтингами (если один из двух попадает - то это засчитывается за верное предсказание)\ntop_2_accuracy, top_2_preds, top_2_probs, _ = eval_model(test_loader, model, top_k=2)\n# точность топ-3 - то же самое, только с топ-3\ntop_3_accuracy, top_3_preds, top_3_probs, _ = eval_model(test_loader, model, top_k=3)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:18:07.715805Z","iopub.execute_input":"2024-10-16T14:18:07.716167Z","iopub.status.idle":"2024-10-16T14:23:34.872570Z","shell.execute_reply.started":"2024-10-16T14:18:07.716134Z","shell.execute_reply":"2024-10-16T14:23:34.871669Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 310/310 [01:51<00:00,  2.79it/s]\nEvaluating: 100%|██████████| 310/310 [01:47<00:00,  2.89it/s]\nEvaluating: 100%|██████████| 310/310 [01:47<00:00,  2.88it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Сделаем несколько примеров для наглядности\nnum_examples = 5\nexample_df = pd.DataFrame({\n    'text': test_df['text'][:num_examples],\n    'actual_label': actual_labels[:num_examples],\n    'top_1_prediction': [list(preds) for preds in top_1_preds[:num_examples]],\n    'top_1_probabilities': [list(probs) for probs in top_1_probs[:num_examples]],\n    'top_2_prediction': [list(preds) for preds in top_2_preds[:num_examples]],\n    'top_2_probabilities': [list(probs) for probs in top_2_probs[:num_examples]],\n    'top_3_prediction': [list(preds) for preds in top_3_preds[:num_examples]],\n    'top_3_probabilities': [list(probs) for probs in top_3_probs[:num_examples]],\n})\n\nprint(\"\\nТочность:\")\nprint(f\"Top-1: {top_1_accuracy:.2%}\")\nprint(f\"Top-2: {top_2_accuracy:.2%}\")\nprint(f\"Top-3: {top_3_accuracy:.2%}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:23:34.874534Z","iopub.execute_input":"2024-10-16T14:23:34.874968Z","iopub.status.idle":"2024-10-16T14:23:34.885046Z","shell.execute_reply.started":"2024-10-16T14:23:34.874920Z","shell.execute_reply":"2024-10-16T14:23:34.884122Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\nТочность:\nTop-1: 4.30%\nTop-2: 28.63%\nTop-3: 39.88%\n","output_type":"stream"}]},{"cell_type":"code","source":"example_df","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:23:38.814839Z","iopub.execute_input":"2024-10-16T14:23:38.815465Z","iopub.status.idle":"2024-10-16T14:23:38.837563Z","shell.execute_reply.started":"2024-10-16T14:23:38.815426Z","shell.execute_reply":"2024-10-16T14:23:38.836772Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                                text  actual_label  \\\n0  What a gargantuan pile of malodorous ordure! Y...             1   \n1  I was first introduced to \"Eddie\" by friends f...            10   \n2  Lexi befriends Jennifer, a thin, intelligent g...            10   \n3  the real plot...<br /><br />A group of post-Ci...             1   \n4  I actually like Asylum movies. I've made it a ...             1   \n\n  top_1_prediction top_1_probabilities top_2_prediction  \\\n0              [0]         [0.9810503]           [0, 1]   \n1              [7]          [0.859163]           [7, 6]   \n2              [4]        [0.32465562]           [4, 5]   \n3              [0]        [0.82128704]           [0, 1]   \n4              [0]        [0.96615577]           [0, 1]   \n\n         top_2_probabilities top_3_prediction  \\\n0   [0.9810503, 0.016247742]        [0, 1, 2]   \n1      [0.859163, 0.0935864]        [7, 6, 5]   \n2   [0.32465562, 0.25848362]        [4, 5, 3]   \n3   [0.82128704, 0.12733063]        [0, 1, 2]   \n4  [0.96615577, 0.028151466]        [0, 1, 2]   \n\n                       top_3_probabilities  \n0   [0.9810503, 0.016247742, 0.0015734542]  \n1        [0.859163, 0.0935864, 0.04039549]  \n2     [0.32465562, 0.25848362, 0.12228855]  \n3     [0.82128704, 0.12733063, 0.03112289]  \n4  [0.96615577, 0.028151466, 0.0037979262]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>actual_label</th>\n      <th>top_1_prediction</th>\n      <th>top_1_probabilities</th>\n      <th>top_2_prediction</th>\n      <th>top_2_probabilities</th>\n      <th>top_3_prediction</th>\n      <th>top_3_probabilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What a gargantuan pile of malodorous ordure! Y...</td>\n      <td>1</td>\n      <td>[0]</td>\n      <td>[0.9810503]</td>\n      <td>[0, 1]</td>\n      <td>[0.9810503, 0.016247742]</td>\n      <td>[0, 1, 2]</td>\n      <td>[0.9810503, 0.016247742, 0.0015734542]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I was first introduced to \"Eddie\" by friends f...</td>\n      <td>10</td>\n      <td>[7]</td>\n      <td>[0.859163]</td>\n      <td>[7, 6]</td>\n      <td>[0.859163, 0.0935864]</td>\n      <td>[7, 6, 5]</td>\n      <td>[0.859163, 0.0935864, 0.04039549]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lexi befriends Jennifer, a thin, intelligent g...</td>\n      <td>10</td>\n      <td>[4]</td>\n      <td>[0.32465562]</td>\n      <td>[4, 5]</td>\n      <td>[0.32465562, 0.25848362]</td>\n      <td>[4, 5, 3]</td>\n      <td>[0.32465562, 0.25848362, 0.12228855]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the real plot...&lt;br /&gt;&lt;br /&gt;A group of post-Ci...</td>\n      <td>1</td>\n      <td>[0]</td>\n      <td>[0.82128704]</td>\n      <td>[0, 1]</td>\n      <td>[0.82128704, 0.12733063]</td>\n      <td>[0, 1, 2]</td>\n      <td>[0.82128704, 0.12733063, 0.03112289]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I actually like Asylum movies. I've made it a ...</td>\n      <td>1</td>\n      <td>[0]</td>\n      <td>[0.96615577]</td>\n      <td>[0, 1]</td>\n      <td>[0.96615577, 0.028151466]</td>\n      <td>[0, 1, 2]</td>\n      <td>[0.96615577, 0.028151466, 0.0037979262]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Вопреки нашим ожиданиям, даже несмотря на то что во время обучения метрика accuracy показывала 50%, здесь результаты значительно хуже. Даже топ-3 не дотягивают до 50%. Вероятно, модель сильно переобучилась. Необходимы дальнейшие эксперименты.  \n\nВвиду ограниченности вермени мы не можем провести дальнейшие опыты с обучением, поэтому в сервис вложим эту модель. Благо, всегда можно обучить новую улучшеную модель и легко всунуть в код сервиса.","metadata":{}},{"cell_type":"markdown","source":"Теперь проверим сентимент.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\ntest_df = pd.read_csv('/kaggle/working/test_reviews.csv')\n\nmodel_name = 'Gnider/roberta_dist_sent'\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nclass SentimentTestDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.dataframe = dataframe\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['text']\n        label_text = self.dataframe.iloc[idx]['label']\n        label = 0 if label_text == 'neg' else 1\n        encodings = self.tokenizer(\n            text, truncation=True, padding='max_length', \n            max_length=self.max_len, return_tensors=\"pt\"\n        )\n        encodings['labels'] = torch.tensor(label, dtype=torch.long)\n        return {key: val.squeeze(0) for key, val in encodings.items()}\n\nmax_len = 512\ntest_dataset = SentimentTestDataset(test_df, tokenizer, max_len)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\ndef evaluate_model(loader, model):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Оценка\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    return accuracy, all_labels, all_preds\n\n#общая оценка точности предсказания сентимента\naccuracy, actual_labels, predicted_labels = evaluate_model(test_loader, model)\nprint(f\"Accuracy: {accuracy:.2%}\")\n\n# 50 примеров\nexample_count = 50\nexamples_df = pd.DataFrame({\n    'text': test_df['text'][:example_count],\n    'actual_label': ['pos' if label == 1 else 'neg' for label in actual_labels[:example_count]],\n    'predicted_label': ['pos' if label == 1 else 'neg' for label in predicted_labels[:example_count]],\n})\n\nexamples_df\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:25:07.227989Z","iopub.execute_input":"2024-10-16T14:25:07.229004Z","iopub.status.idle":"2024-10-16T14:27:01.929598Z","shell.execute_reply.started":"2024-10-16T14:25:07.228959Z","shell.execute_reply":"2024-10-16T14:27:01.928768Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895661c244454ebf9de372f6a1433855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02715d6631e543c6b7dd07f90654261e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ad5c45be7e4fda9a480f35e241f4b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37a29d18a3b246fe95119d60a8cb11b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/751 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401adc760c9b42be8605c55977fac597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd96459ee7f4f278953210a21d3e5b0"}},"metadata":{}},{"name":"stderr","text":"Оценка: 100%|██████████| 310/310 [01:50<00:00,  2.79it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 98.60%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                                 text actual_label  \\\n0   What a gargantuan pile of malodorous ordure! Y...          neg   \n1   I was first introduced to \"Eddie\" by friends f...          pos   \n2   Lexi befriends Jennifer, a thin, intelligent g...          pos   \n3   the real plot...<br /><br />A group of post-Ci...          neg   \n4   I actually like Asylum movies. I've made it a ...          neg   \n5   I really liked this movie. If other people wan...          pos   \n6   **** Possible Spoiler **** <br /><br />If you ...          neg   \n7   Oh man, I know what your thinking: \"With a tit...          neg   \n8   Never viewed this film and enjoyed the singing...          pos   \n9   Maiden Voyage is just that. I'd like to say st...          neg   \n10  Unfortunately, this film is typical of the wat...          neg   \n11  This film was hilarious. It provided a somewha...          pos   \n12  The emotional powers and characters of Dominic...          pos   \n13  I just finished reading a book about Dillinger...          neg   \n14  After witnessing his wife (Linda Hoffman) enga...          pos   \n15  I love dogs, and the most interesting characte...          pos   \n16  as can be read in many reviews here it is a mo...          pos   \n17  CQ was the worst film I saw this year. Nearly ...          neg   \n18  I have seen all the films directed by Robert R...          pos   \n19  Sometimes there's a film so bad that you just ...          neg   \n20  Some of the best movies that are categorized a...          pos   \n21  One wonders how FLYNN could have failed so bad...          neg   \n22  Back when musicals weren't showcases for chore...          pos   \n23  The comments already left for this show are wa...          neg   \n24  Evan Almighty continues the mainstream Bruce A...          neg   \n25  by TyNesha Mells. In this drama, Ja Rule, who ...          pos   \n26  If you like movies about creepy towns, hotels,...          neg   \n27  Think Jumanji but with a death curse. A bunch ...          neg   \n28  I'm watching the series again now that it's ou...          pos   \n29  Absolutely one of my favorite movies of all ti...          pos   \n30  John Voight plays the title character in this ...          pos   \n31  the reason why i gave this movie a 4 was for a...          neg   \n32  I don't expect a lot from ghost stories, but I...          neg   \n33  Sure, this film was retarded. But you expected...          neg   \n34  Guys and Dolls is a unique play based on the c...          pos   \n35  Is there any question that Jeffrey Combs is on...          pos   \n36  I really only watched this movie because it ha...          pos   \n37  This is without a doubt the most poorly though...          neg   \n38  There are bad movies, terrible movies even bor...          neg   \n39  I went into the movie expecting a little actio...          neg   \n40  I'd have to say that I've seen worse Sci Fi Ch...          neg   \n41  Predictable Unmotivated Pointless Caricatures ...          neg   \n42  This film is great - well written and very ent...          pos   \n43  My mate and I chose to watch this obvious piec...          neg   \n44  I like this movie because it is a fine work of...          pos   \n45  The critics didn't like this film. It bombed i...          pos   \n46  Many of these other viewers complain that the ...          pos   \n47  Me and a group of friends rent horrible videos...          neg   \n48  I completely forgot that I'd seen this within ...          neg   \n49  This film has got so much in it. Prehistoric s...          neg   \n\n   predicted_label  \n0              neg  \n1              pos  \n2              pos  \n3              neg  \n4              neg  \n5              pos  \n6              neg  \n7              neg  \n8              pos  \n9              neg  \n10             neg  \n11             pos  \n12             pos  \n13             neg  \n14             pos  \n15             pos  \n16             pos  \n17             neg  \n18             pos  \n19             neg  \n20             pos  \n21             neg  \n22             pos  \n23             neg  \n24             neg  \n25             pos  \n26             neg  \n27             neg  \n28             pos  \n29             pos  \n30             pos  \n31             neg  \n32             neg  \n33             neg  \n34             pos  \n35             pos  \n36             pos  \n37             neg  \n38             neg  \n39             neg  \n40             neg  \n41             neg  \n42             pos  \n43             neg  \n44             pos  \n45             pos  \n46             pos  \n47             neg  \n48             neg  \n49             neg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>actual_label</th>\n      <th>predicted_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What a gargantuan pile of malodorous ordure! Y...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I was first introduced to \"Eddie\" by friends f...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lexi befriends Jennifer, a thin, intelligent g...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the real plot...&lt;br /&gt;&lt;br /&gt;A group of post-Ci...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I actually like Asylum movies. I've made it a ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I really liked this movie. If other people wan...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>**** Possible Spoiler **** &lt;br /&gt;&lt;br /&gt;If you ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Oh man, I know what your thinking: \"With a tit...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Never viewed this film and enjoyed the singing...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Maiden Voyage is just that. I'd like to say st...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Unfortunately, this film is typical of the wat...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>This film was hilarious. It provided a somewha...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The emotional powers and characters of Dominic...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>I just finished reading a book about Dillinger...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>After witnessing his wife (Linda Hoffman) enga...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>I love dogs, and the most interesting characte...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>as can be read in many reviews here it is a mo...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>CQ was the worst film I saw this year. Nearly ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I have seen all the films directed by Robert R...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Sometimes there's a film so bad that you just ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Some of the best movies that are categorized a...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>One wonders how FLYNN could have failed so bad...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Back when musicals weren't showcases for chore...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>The comments already left for this show are wa...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Evan Almighty continues the mainstream Bruce A...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>by TyNesha Mells. In this drama, Ja Rule, who ...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>If you like movies about creepy towns, hotels,...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Think Jumanji but with a death curse. A bunch ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>I'm watching the series again now that it's ou...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Absolutely one of my favorite movies of all ti...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>John Voight plays the title character in this ...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>the reason why i gave this movie a 4 was for a...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>I don't expect a lot from ghost stories, but I...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Sure, this film was retarded. But you expected...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Guys and Dolls is a unique play based on the c...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Is there any question that Jeffrey Combs is on...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>I really only watched this movie because it ha...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>This is without a doubt the most poorly though...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>There are bad movies, terrible movies even bor...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>I went into the movie expecting a little actio...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>I'd have to say that I've seen worse Sci Fi Ch...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Predictable Unmotivated Pointless Caricatures ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>This film is great - well written and very ent...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>My mate and I chose to watch this obvious piec...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>I like this movie because it is a fine work of...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>The critics didn't like this film. It bombed i...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Many of these other viewers complain that the ...</td>\n      <td>pos</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Me and a group of friends rent horrible videos...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>I completely forgot that I'd seen this within ...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>This film has got so much in it. Prehistoric s...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Здесь все значительно лучше: точность предсказания сентимента - 98.6%. Из примеров все 50 сентиментов предсказаны верно.\n","metadata":{}},{"cell_type":"markdown","source":"Следующий шаг - оформление сервиса Django и развертывание. Подробнее об этом - в файле отчета.","metadata":{}}]}